{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b6bcde1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym \n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch.distributions.categorical import Categorical\n",
    "\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4a79a46c",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8288dba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Policy and Value Models\n",
    "class ActorCriticNetwork:\n",
    "    def __init__(self, obs_space_size, action_space_size):\n",
    "        super().__init__()\n",
    "        self.shared_layers = nn.Sequential(\n",
    "            nn.Linear(obs_space_size, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64,64),\n",
    "            nn.ReLU())\n",
    "        \n",
    "        self.policy_layers = nn.Sequential(\n",
    "            nn.Linear(64,64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64,action_space_size))\n",
    "        \n",
    "        self.value_layers = nn.Sequential(\n",
    "            nn.Linear(64,64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64,1))\n",
    "        \n",
    "    def value(self, obs):\n",
    "        z = self.shared_layers(obs)\n",
    "        value = self.value_layers(z)\n",
    "        return value\n",
    "    \n",
    "    def policy(self, obs):\n",
    "        z = self.shared_layers(obs)\n",
    "        policy_logits = self.policy_layers(z)\n",
    "        return policy_logits\n",
    "    \n",
    "    def forward(self, obs):\n",
    "        z = shared_layers(obs)\n",
    "        policy_logits = policy_layers(z)\n",
    "        value = value_layers(z)\n",
    "        return policy_logits, value\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a3daf416",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Create Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "81d48bb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Create Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9efc0e4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rollout(model, env, max_steps=1000):\n",
    "    \"\"\"\n",
    "    Performs a single rollout.\n",
    "    Returns training data in the shape (n_steps, observation_shape)\n",
    "    and the cumulative reward.\n",
    "    \"\"\"\n",
    "    # Create Data Storage\n",
    "    train_data = []\n",
    "    obs = env.reset()\n",
    "    \n",
    "    ep_reward = 0\n",
    "    for _ in range(max_steps):\n",
    "        logits, val = model(torch.tensor([obs], dtype=torch.float32, device = DEVICE))\n",
    "        act_distribution = Categorical(logits=logits)\n",
    "        act = act_distribution.sample()\n",
    "        act_log_prob = act_distribution.log_prob(act).item()\n",
    "        next_obs, reward, done, _ = env.step(act.item())\n",
    "        \n",
    "        # Record Data for training\n",
    "        \n",
    "        \n",
    "        obs = next_obs\n",
    "        ep_reward += reward\n",
    "        if done:\n",
    "            break\n",
    "    return train_data, ep_reward\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f6944159",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'ActorCriticNetwork' object has no attribute 'to'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[1;32mIn [11]\u001b[0m, in \u001b[0;36m<cell line: 4>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m env \u001b[38;5;241m=\u001b[39m gym\u001b[38;5;241m.\u001b[39mmake(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCartPole-v0\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      3\u001b[0m model \u001b[38;5;241m=\u001b[39m ActorCriticNetwork(env\u001b[38;5;241m.\u001b[39mobservation_space\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], env\u001b[38;5;241m.\u001b[39maction_space\u001b[38;5;241m.\u001b[39mn)\n\u001b[1;32m----> 4\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m(DEVICE)\n\u001b[0;32m      5\u001b[0m train_data, reward \u001b[38;5;241m=\u001b[39m rollout(model, env)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'ActorCriticNetwork' object has no attribute 'to'"
     ]
    }
   ],
   "source": [
    "## Environment Setup\n",
    "env = gym.make('CartPole-v0')\n",
    "model = ActorCriticNetwork(env.observation_space.shape[0], env.action_space.n)\n",
    "model = model.to(DEVICE)\n",
    "train_data, reward = rollout(model, env) # Test rollout Funtion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c45295fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Init PPO trainer and params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e13917b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Training params\n",
    "n_episodes = 200\n",
    "print_freq = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0cd1577",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Loop\n",
    "ep_rewards = []\n",
    "for episode_idx in range(n_episodes):\n",
    "    # Perform rollout\n",
    "    train_data, reward = rollout(model, env)\n",
    "    ep_rewards.append(reward)\n",
    "    # Data formarting\n",
    "    \n",
    "    # Policy + Vale Network Training\n",
    "    if (episode_idx + 1) % print_freq == 0:\n",
    "        print('Episode {} | Avg Reward {:.1f}'.format(episode_idx + 1, np.mean(ep_rewards[-print_freq:])))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
