{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b6bcde1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym \n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch.distributions.categorical import Categorical\n",
    "\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4a79a46c",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8288dba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Policy and value model\n",
    "class ActorCriticNetwork(nn.Module):\n",
    "    def __init__(self, obs_space_size, action_space_size):\n",
    "        super().__init__()\n",
    "\n",
    "        self.shared_layers = nn.Sequential(\n",
    "            nn.Linear(obs_space_size, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 64),\n",
    "            nn.ReLU())\n",
    "\n",
    "        self.policy_layers = nn.Sequential(\n",
    "            nn.Linear(64, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, action_space_size))\n",
    "\n",
    "        self.value_layers = nn.Sequential(\n",
    "            nn.Linear(64, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 1))\n",
    "    \n",
    "    def value(self, obs):\n",
    "        z = self.shared_layers(obs)\n",
    "        value = self.value_layers(z)\n",
    "        return value\n",
    "\n",
    "    def policy(self, obs):\n",
    "        z = self.shared_layers(obs)\n",
    "        policy_logits = self.policy_layers(z)\n",
    "        return policy_logits\n",
    "\n",
    "    def forward(self, obs):\n",
    "        z = self.shared_layers(obs)\n",
    "        policy_logits = self.policy_layers(z)\n",
    "        value = self.value_layers(z)\n",
    "        return policy_logits, value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a3daf416",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PPOTrainer():\n",
    "    def __init__(self,\n",
    "                  actor_critic,\n",
    "                  ppo_clip_val=0.2,\n",
    "                  target_kl_div=0.01,\n",
    "                  max_policy_train_iters=80,\n",
    "                  value_train_iters=80,\n",
    "                  policy_lr=3e-4,\n",
    "                  value_lr=1e-2):\n",
    "        self.ac = actor_critic\n",
    "        self.ppo_clip_val = ppo_clip_val\n",
    "        self.target_kl_div = target_kl_div\n",
    "        self.max_policy_train_iters = max_policy_train_iters\n",
    "        self.value_train_iters = value_train_iters\n",
    "\n",
    "        policy_params = list(self.ac.shared_layers.parameters()) + \\\n",
    "            list(self.ac.policy_layers.parameters())\n",
    "        self.policy_optim = optim.Adam(policy_params, lr=policy_lr)\n",
    "\n",
    "        value_params = list(self.ac.shared_layers.parameters()) + \\\n",
    "            list(self.ac.value_layers.parameters())\n",
    "        self.value_optim = optim.Adam(value_params, lr=value_lr)\n",
    "\n",
    "    def train_policy(self, obs, acts, old_log_probs, gaes):\n",
    "        for _ in range(self.max_policy_train_iters):\n",
    "            self.policy_optim.zero_grad()\n",
    "\n",
    "            new_logits = self.ac.policy(obs)\n",
    "            new_logits = Categorical(logits=new_logits)\n",
    "            new_log_probs = new_logits.log_prob(acts)\n",
    "\n",
    "            policy_ratio = torch.exp(new_log_probs - old_log_probs)\n",
    "            clipped_ratio = policy_ratio.clamp(\n",
    "              1 - self.ppo_clip_val, 1 + self.ppo_clip_val)\n",
    "\n",
    "            clipped_loss = clipped_ratio * gaes\n",
    "            full_loss = policy_ratio * gaes\n",
    "            policy_loss = -torch.min(full_loss, clipped_loss).mean()\n",
    "\n",
    "            policy_loss.backward()\n",
    "            self.policy_optim.step()\n",
    "\n",
    "            kl_div = (old_log_probs - new_log_probs).mean()\n",
    "            if kl_div >= self.target_kl_div:\n",
    "                break\n",
    "\n",
    "    def train_value(self, obs, returns):\n",
    "        for _ in range(self.value_train_iters):\n",
    "            self.value_optim.zero_grad()\n",
    "\n",
    "            values = self.ac.value(obs)\n",
    "            value_loss = (returns - values) ** 2\n",
    "            value_loss = value_loss.mean()\n",
    "\n",
    "            value_loss.backward()\n",
    "            self.value_optim.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "81d48bb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def discount_rewards(rewards, gamma=0.99):\n",
    "    \"\"\"\n",
    "    Return discounted rewards based on the given rewards and gamma param.\n",
    "    \"\"\"\n",
    "    new_rewards = [float(rewards[-1])]\n",
    "    for i in reversed(range(len(rewards)-1)):\n",
    "        new_rewards.append(float(rewards[i]) + gamma * new_rewards[-1])\n",
    "    return np.array(new_rewards[::-1])\n",
    "\n",
    "def calculate_gaes(rewards, values, gamma=0.99, decay=0.97):\n",
    "    \"\"\"\n",
    "    Return the General Advantage Estimates from the given rewards and values.\n",
    "    Paper: https://arxiv.org/pdf/1506.02438.pdf\n",
    "    \"\"\"\n",
    "    next_values = np.concatenate([values[1:], [0]])\n",
    "    deltas = [rew + gamma * next_val - val for rew, val, next_val in zip(rewards, values, next_values)]\n",
    "\n",
    "    gaes = [deltas[-1]]\n",
    "    for i in reversed(range(len(deltas)-1)):\n",
    "        gaes.append(deltas[i] + decay * gamma * gaes[-1])\n",
    "\n",
    "    return np.array(gaes[::-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9efc0e4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rollout(model, env, max_steps=1000):\n",
    "    \"\"\"\n",
    "    Performs a single rollout.\n",
    "    Returns training data in the shape (n_steps, observation_shape)\n",
    "    and the cumulative reward.\n",
    "    \"\"\"\n",
    "    ### Create data storage\n",
    "    train_data = [[], [], [], [], []] # obs, act, reward, values, act_log_probs\n",
    "    obs = env.reset()\n",
    "\n",
    "    ep_reward = 0\n",
    "    for _ in range(max_steps):\n",
    "        logits, val = model(torch.tensor([obs], dtype=torch.float32,\n",
    "                                         device=DEVICE))\n",
    "        act_distribution = Categorical(logits=logits)\n",
    "        act = act_distribution.sample()\n",
    "        act_log_prob = act_distribution.log_prob(act).item()\n",
    "\n",
    "        act, val = act.item(), val.item()\n",
    "\n",
    "        next_obs, reward, done, _ = env.step(act)\n",
    "\n",
    "        for i, item in enumerate((obs, act, reward, val, act_log_prob)):\n",
    "          train_data[i].append(item)\n",
    "\n",
    "        obs = next_obs\n",
    "        ep_reward += reward\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    train_data = [np.asarray(x) for x in train_data]\n",
    "\n",
    "    ### Do train data filtering\n",
    "    train_data[3] = calculate_gaes(train_data[2], train_data[3])\n",
    "\n",
    "    return train_data, ep_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f6944159",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Environment Setup\n",
    "env = gym.make('CartPole-v0')\n",
    "model = ActorCriticNetwork(env.observation_space.shape[0], env.action_space.n)\n",
    "model = model.to(DEVICE)\n",
    "train_data, reward = rollout(model, env) # Test rollout Funtion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6e947139",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13.05146046 12.55602055 12.027083   11.47502388 10.90507021 10.30973848\n",
      "  9.69991264  9.06252657  8.39277336  7.70148848  6.9752633   6.22542016\n",
      "  5.43806335  4.6160322   3.75944526  2.86748356  1.94996276  0.99373018]\n"
     ]
    }
   ],
   "source": [
    "print(train_data[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e13917b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define training params\n",
    "n_episodes = 200\n",
    "print_freq = 20\n",
    "\n",
    "ppo = PPOTrainer(\n",
    "    model,\n",
    "    policy_lr = 3e-4,\n",
    "    value_lr = 1e-3,\n",
    "    target_kl_div = 0.02,\n",
    "    max_policy_train_iters = 40,\n",
    "    value_train_iters = 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b0cd1577",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 20 | Avg Reward 13.4\n",
      "Episode 40 | Avg Reward 12.6\n",
      "Episode 60 | Avg Reward 11.9\n",
      "Episode 80 | Avg Reward 16.6\n",
      "Episode 100 | Avg Reward 23.4\n",
      "Episode 120 | Avg Reward 61.0\n",
      "Episode 140 | Avg Reward 123.1\n",
      "Episode 160 | Avg Reward 161.3\n",
      "Episode 180 | Avg Reward 184.8\n",
      "Episode 200 | Avg Reward 183.3\n"
     ]
    }
   ],
   "source": [
    "# Training Loop\n",
    "ep_rewards = []\n",
    "for episode_idx in range(n_episodes):\n",
    "    # Perform rollout\n",
    "    train_data, reward = rollout(model, env)\n",
    "    ep_rewards.append(reward)\n",
    "    \n",
    "    #Shuffle\n",
    "    permute_idxs = np.random.permutation(len(train_data[0]))\n",
    "    \n",
    "    # Policy Data\n",
    "    obs = torch.tensor(train_data[0][permute_idxs], \n",
    "                       dtype=torch.float32, device=DEVICE)\n",
    "    acts = torch.tensor(train_data[1][permute_idxs],\n",
    "                       dtype=torch.int32, device=DEVICE)\n",
    "    gaes = torch.tensor(train_data[3][permute_idxs], \n",
    "                       dtype=torch.float32, device=DEVICE)\n",
    "    act_log_probs = torch.tensor(train_data[4][permute_idxs],\n",
    "                                dtype=torch.float32, device=DEVICE)\n",
    "    \n",
    "    # Value Data\n",
    "    returns = discount_rewards(train_data[2][permute_idxs])\n",
    "    returns = torch.tensor(returns, dtype=torch.float32, device=DEVICE)\n",
    "    \n",
    "    # Train Model\n",
    "    ppo.train_policy(obs, acts, act_log_probs, gaes)\n",
    "    ppo.train_value(obs, returns)\n",
    "    \n",
    "    # Policy + Vale Network Training\n",
    "    if (episode_idx + 1) % print_freq == 0:\n",
    "        print('Episode {} | Avg Reward {:.1f}'.format(episode_idx + 1, np.mean(ep_rewards[-print_freq:])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "209e81cf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f402bd7c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
